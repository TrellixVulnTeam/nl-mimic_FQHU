{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text predictions with Markov Chains v2\n",
    "Max Fierro 8/15/2022\n",
    "\n",
    "### Motivation\n",
    "After revisiting this little exercise some months later, I found some improvements and different perspectives from which I can see this problem. Namely, I realized that creating these 'pair links' is essentially creating a directed graph where each word is a vertex, and consecutive words have an edge between them. This gives a natural explanation to the cyclic behavior we were seeing with the deterministic chains -- words would have strong cycles between them by referencing each other with the highest probability. \n",
    "\n",
    "A quick google search confirms that Markov Chains are indeed directed graphs, where each node represents some state. Who would have known, huh? This was truly staring at me right in the face, as in the last part of this exercise I attempted to use what is basically an adjacency matrix to represent these relationships. This time, I will use an adjacency dictionary of type `String : Array<String>` to represent this digraph, and hopefully have some cooler results by using more, uh, 'insightful' algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the first job will be to tokenize the words of our favorite book of questionable language. This is fairly self explanatory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_words(file, unwanted, chars = None):\n",
    "    doc = open(file)\n",
    "    data = doc.read(chars).split()\n",
    "    doc.close()\n",
    "    words = []\n",
    "    for word in data:\n",
    "        for item in unwanted:\n",
    "            word = word.replace(item, '')\n",
    "        word = word.lower()\n",
    "        words.append(word)\n",
    "    return words\n",
    "\n",
    "unwanted = ['\"', '^', '*', '-', '_', '/', '[', ']', '<', '>', '~', ',']\n",
    "\n",
    "words = read_words('data/huck_finn.txt', unwanted, 1_000_000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we construct a digraph, where instead of having the edges be weighted, we just have repeats in the array which the keys map to. This way, we can have random lookups within the arrays, and have the right probability of obtaining each next word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "digraph = {}\n",
    "\n",
    "for index in range(len(words) - 1):\n",
    "    \n",
    "    current_word = words[index]\n",
    "    next_word = words[index + 1]\n",
    "\n",
    "    if current_word not in digraph:\n",
    "        digraph[current_word] = []\n",
    "    \n",
    "    digraph[current_word].append(next_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right. Last time I had the right idea with making the individual links, but I was basically reinventing the wheel (but in a dumb way, so more like the square wheel). Now things are much simpler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "protection; thought of the right along. he could reform the robbers; but you want of my whole crowd the early in my lessons good. he says: i slip over his name the raft again to the crick amongst the bills said: chapter xiv. by and tell me all right then; at him and six foot a dime and change or a shoutand then broke bekase why: would take the king and cave opened his pardon. she has cut loose himself; but there was worth of the truck in front of the lightningrod and feeling pretty dull. buck looked sorrowful and \n",
      "\n",
      " Calculated in: 0.00028586387634277344\n"
     ]
    }
   ],
   "source": [
    "first_word = random.choice(words)\n",
    "prediction_length = 100\n",
    "prediction = \"\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "while prediction_length > 0:\n",
    "    prediction += first_word + \" \"\n",
    "    first_word = random.choice(digraph[first_word])\n",
    "    prediction_length -= 1\n",
    "\n",
    "finish = time.time()\n",
    "time_elapsed = finish - start\n",
    "\n",
    "print(prediction + \"\\n\\n Calculated in: \" + str(time_elapsed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is the kind of performance I was expecting from the matrix representation I made in the last exercise. With some preprocessing, we can get predictions within time linear in the amount of words we want, which is very noice.\n",
    "\n",
    "Overall, this solution is more elegant and correct than my last one, so I will call it a day once more. \n",
    "\n",
    "Improvement, people, improvement."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4f4b22bfa6fe1b5fb31716095531c980ed5d6800caf0f86448db74c1688e92e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
